{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install transformers[torch]\n",
        "# ! pip install datasets\n",
        "# ! pip install optuna\n",
        "# ! pip3 install -q -U bitsandbytes==0.42.0\n",
        "# ! pip3 install -q -U peft==0.8.2\n",
        "# ! pip3 install -q -U trl==0.7.10"
      ],
      "metadata": {
        "id": "fGYRz0SzJVSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "54074be8-b32c-40c4-cd01-a1d5b27bc26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (2.2.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers[torch]) (0.30.1)\n",
            "Requirement already satisfied: psutil in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (2.11.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers[torch]) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from jinja2->torch->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: datasets in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (2.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas->datasets) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: optuna in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import sklearn\n",
        "from transformers import BitsAndBytesConfig,AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction, EarlyStoppingCallback\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import bitsandbytes as bnb\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from datasets import load_dataset, load_metric\n",
        "from datasets import Dataset\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "# Suppress the SettingWithCopyWarning\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "z8ccD0utT4tO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77a20b9-6ad6-4d51-dc6b-8d32cd8310ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recognizing Textual Entailment (RTE)\n",
        "\n",
        "The RTE task is a natural language processing (NLP) task that involves determining whether one piece of text logically entails another.<br>\n",
        "In our case, we will handle pairs of sentences indicated by the features text1 and text2 to verify if there is an **implication** between the first and the second. <br> Since the RTE task is supervised, the 'label_text' feature provides us with the label assigned for such pairs of sentences. Finally, 'idx' merely indicates the row index of the dataset."
      ],
      "metadata": {
        "id": "iAAMgAFUrfKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the RTE dataset\n",
        "dataset = load_dataset(\"SetFit/rte\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "MEKmVpXBT56g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47aefeca-27f6-496e-a3ea-17bfbd426aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
            "        num_rows: 2490\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
            "        num_rows: 277\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset labels\n",
        "* **entailment** means that one sentence logically follows from another.\n",
        "* **not entailment** means that there's no logical connection between two sentences."
      ],
      "metadata": {
        "id": "oV48kXQusVLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.Series(dataset[\"train\"][\"label_text\"])\n",
        "labels.value_counts().plot(kind = 'bar', color = ['blue','red'],edgecolor = \"black\")\n",
        "\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.title(\"SetFit/rte labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "0opz-z7sX-iD",
        "outputId": "8c30a4aa-79ce-40fb-9aa9-16b13b771213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'SetFit/rte labels')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAFWCAYAAACYQLF6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdvklEQVR4nO3de5QlZX3u8e/DcBGMgMI4wAwyKKhBvGEr5BgvkSMKJKIRCcaEixyJOZpEcSWCccXLORo9iddoElEUjAZFUcEEo4hRkyjogEQQJI4igQGGERCQ++V3/qhq2dN0T+259K7u3t/PWnt11VtVu37da3c//dZbl1QVkiSty2Z9FyBJmvsMC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQ1iHJy5J8pecanp3kqiHXPSrJv2/gfjZ4Wy18hoXmrSS/nuRbSW5KckOS/0jy1CG3rSR7DMw/O8l9SX4x8PpiVX2yqg6YabuB9hOSvH2GfQ39x16aqzbvuwBpQyTZFvgn4A+B04AtgWcAd27E215dVcs2cNuDgeOnNibxd0wLgj0LzVePBqiqU6vq3qq6vaq+UlXfn1whycuTXJrkxiRfTrJb2/7NdpX/bHsQvzPTTgYPzcy0XZKHtvV8e7IXkeT1Sa4FTgW+BOwy0GPZJclmSY5P8uMk1yc5LcnDhvnGB7a7JcklSV70wFXygbbH9cMk+w8s2C7JSUmuSbIqyf9NsmiafSTJe5Jcl+TmJBcl2XuY+rQwGRaar/4LuDfJKUkObP9g/1KSQ4A3AL8NLAb+jeYPN1X1zHa1J1bVr1TVp4fZ4Tq2ex5wTlXd287vBDwM2A04AjiQptfyK+3rauCPgBcCzwJ2AW4EPjjk9/5jml7UdsBbgE8k2Xlg+b7tOjsCbwI+NxBEJwP3AHsATwYOAP7XNPs4AHgmTQhuBxwGXD9kfVqADAvNS1V1M/DrQAEfBtYkOTPJknaVVwJ/WVWXVtU9wNuBJ032LmawS5KfD7wOG7Kcg4GzBubvA95UVXdW1e0zbPNK4M+r6qqquhN4M3DoMIetquozVXV1Vd3XBtaPgKcNrHId8N6qurtdfhlwcPuzOQh4TVXdWlXXAe8BDp9mN3cDDwEeC6T9OV7TVZsWLsNC81b7B+yodpxhb5r/0N/bLt4NeN/kH37gBiDA0nW85dVVtf3A67SuGpJsBjwX+JeB5jVVdUfHprsBnx+o71LgXmDJOrdq9nlEkgsHtt2bphcxaVWtfTvpK2h+NrsBWwDXDGz7IeDhU/dRVV8DPkDT27kuyYntOJHGlGGhBaGqfkhziGXyuPqVwB9M+eO/dVV9axPv+qnAFVW1ZrCcqeVNs92VwIFT6ntQVa1a187antGHgVcDO1TV9sDFNEE4aWmSwflHAFe3+7wT2HFgn9tW1eOm21dVvb+qngLsRXM46k/XVZsWNsNC81KSxyZ5XZJl7fyuwEuBc9tV/h44Icnj2uXbJXnJwFusBh65Abueut1BwD8Psc0OSbYbaPt74G0Dg+6L23GWLg+mCZ817XZHc39ATno48MdJtmi/518FzmoPI30FeFeSbdtB9kcledbUnSR5apJ9k2wB3ArcQXN4TWPKsNB8dQvNQO55SW6lCYmLgdcBVNXngXcCn0pyc7vswIHt3wycsp5jE9NtN3W84gHaXs+pwE/a7XYB3gecCXwlyS1t/ft27byqLgHeBXybJoQeD/zHlNXOA/YEfga8DTi0qiYHp4+gOc34EppB9c8CO/NA29L0YG6kOYx1PfBXXfVp4YpPypM2TDtg/D1gafmLpAXOnoW04bYDXmdQaBzYs5AkdbJnIUnqZFhIkjotyJuc7bjjjrV8+fK+y5CkeeX888//WVUtnm7ZggyL5cuXs2LFir7LkKR5JckVMy3zMJQkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE4L8qK8+WKnnZazevWM18BoPS1ZshvXXvvTvsuQFiTDokdNUHjX301l9ep0r6ShLd9pJ65YvbrvMhaE3ZYs4afXXtt3GRvFsJA0rStWr/ZfmU0kCyB0HbOQJHUyLCRJnWYtLJJ8NMl1SS4eaPurJD9M8v0kn0+y/cCyE5KsTHJZkucNtD+/bVuZ5PjZqleSNLPZ7FmcDDx/StvZwN5V9QTgv4ATAJLsBRwOPK7d5m+TLEqyCPggcCCwF/DSdl1J0gjNWlhU1TeBG6a0faWq7mlnzwWWtdOHAJ+qqjur6nJgJfC09rWyqn5SVXcBn2rXlSSNUJ9jFi8HvtROLwWuHFh2Vds2U/sDJDk2yYokK9asWTML5UrS+OolLJL8OXAP8MlN9Z5VdWJVTVTVxOLF0z4VUJK0gUZ+nUWSo4DfBPavqsnTuFcBuw6stqxtYx3tkqQRGWnPIsnzgT8DXlBVtw0sOhM4PMlWSXYH9gS+A3wX2DPJ7km2pBkEP3OUNUuSZrFnkeRU4NnAjkmuAt5Ec/bTVsDZSQDOrapXVtUPkpwGXEJzeOpVVXVv+z6vBr4MLAI+WlU/mK2aJUnTy/1HghaOiYmJWrFiRd9ldGoCc+H9/PsTFuLnuS9J/HRuIoF58dlMcn5VTUy3zCu4JUmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdZq1sEjy0STXJbl4oO1hSc5O8qP260Pb9iR5f5KVSb6fZJ+BbY5s1/9RkiNnq15J0sxms2dxMvD8KW3HA+dU1Z7AOe08wIHAnu3rWODvoAkX4E3AvsDTgDdNBowkaXRmLSyq6pvADVOaDwFOaadPAV440P7xapwLbJ9kZ+B5wNlVdUNV3QiczQMDSJI0y0Y9ZrGkqq5pp68FlrTTS4ErB9a7qm2bqf0BkhybZEWSFWvWrNm0VUvSmOttgLuqCqhN+H4nVtVEVU0sXrx4U72tJInRh8Xq9vAS7dfr2vZVwK4D6y1r22ZqlySN0KjD4kxg8oymI4EzBtqPaM+K2g+4qT1c9WXggCQPbQe2D2jbJEkjtPlsvXGSU4FnAzsmuYrmrKZ3AKclOQa4AjisXf0s4CBgJXAbcDRAVd2Q5P8A323Xe2tVTR00lyTNsjRDBwvLxMRErVixou8yOiVhEw7biLAQP899SeKncxMJzIvPZpLzq2piumVewS1J6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKlTL2GR5LVJfpDk4iSnJnlQkt2TnJdkZZJPJ9myXXerdn5lu3x5HzVL0jgbeVgkWQr8MTBRVXsDi4DDgXcC76mqPYAbgWPaTY4Bbmzb39OuJ0kaob4OQ20ObJ1kc2Ab4BrgOcBn2+WnAC9spw9p52mX758koytVkjTysKiqVcBfA/9NExI3AecDP6+qe9rVrgKWttNLgSvbbe9p199hlDVL0rgbKiySPH5T7TDJQ2l6C7sDuwAPBp6/Cd732CQrkqxYs2bNxr6dJGnAsD2Lv03ynST/O8l2G7nP/wlcXlVrqupu4HPA04Ht28NSAMuAVe30KmBXgHb5dsD1U9+0qk6sqomqmli8ePFGlihJGjRUWFTVM4CX0fzRPj/JPyZ57gbu87+B/ZJs04497A9cAvwrcGi7zpHAGe30me087fKvVVVt4L4lSRtg6DGLqvoR8Ebg9cCzgPcn+WGS316fHVbVeTQD1RcAF7U1nNi+73FJVtKMSZzUbnISsEPbfhxw/PrsT5K08TLMP+lJngAcDRwMnA2cVFUXJNkF+HZV7Ta7Za6fiYmJWrFiRd9ldGo6VnaSNp1gp3PTSeKncxMJzIvPZpLzq2piumWbT9c4jb8BPgK8oapun2ysqquTvHET1ChJmsOGDYuDgdur6l6AJJsBD6qq26rqH2atOknSnDDsmMVXga0H5rdp2yRJY2DYsHhQVf1icqad3mZ2SpIkzTXDhsWtSfaZnEnyFOD2dawvSVpAhh2zeA3wmSRX0wzs7wT8zmwVJUmaW4YKi6r6bpLHAo9pmy5rr76WJI2BYXsWAE8Flrfb7JOEqvr4rFQlSZpThgqLJP8APAq4ELi3bS7AsJCkMTBsz2IC2Mt7MknSeBr2bKiLaQa1JUljaNiexY7AJUm+A9w52VhVL5iVqiRJc8qwYfHm2SxCkjS3DXvq7DeS7AbsWVVfTbINsGh2S5MkzRXDPlb1FTTPoPhQ27QU+MIs1SRJmmOGHeB+Fc2jT2+GXz4I6eGzVZQkaW4ZNizurKq7JmfaZ2F7Gq0kjYlhw+IbSd4AbN0+e/szwBdnryxJ0lwybFgcD6yheWb2HwBn0TyPW5I0BoY9G+o+4MPtS5I0Zoa9N9TlTDNGUVWP3OQVSZLmnPW5N9SkBwEvAR626cuRJM1FQ41ZVNX1A69VVfVe4ODZLU2SNFcMexhqn4HZzWh6GuvzLAxJ0jw27B/8dw1M3wP8FDhsQ3eaZHvgI8DeNGMhLwcuAz5N84ClnwKHVdWNSQK8DzgIuA04qqou2NB9S5LW37BnQ/3GJt7v+4B/qapDk2wJbAO8ATinqt6R5Hia03VfDxwI7Nm+9gX+rv0qSRqRYQ9DHbeu5VX17mF3mGQ74JnAUe22dwF3JTkEeHa72inA12nC4hDg4+2Dl85Nsn2SnavqmmH3KUnaOMNelDcB/CHNDQSXAq8E9gEe0r7Wx+40F/h9LMn3knwkyYOBJQMBcC2wpJ1eClw5sP1VbZskaUSGHbNYBuxTVbcAJHkz8M9V9XsbuM99gD+qqvOSvI/mkNMvVVUlWa97TyU5FjgW4BGPeMQGlCVJmsmwPYslwF0D83dx/3/+6+sq4KqqOq+d/yxNeKxOsjNA+/W6dvkqYNeB7Ze1bWupqhOraqKqJhYvXryBpUmSpjNsWHwc+E6SN7e9ivNoxhXWW1VdC1yZ5DFt0/7AJcCZwJFt25HAGe30mcARaewH3OR4hSSN1rBnQ70tyZeAZ7RNR1fV9zZiv38EfLI9E+onwNE0wXVakmOAK7j/1NyzaE6bXUlz6uzRG7FfSdIGWJ8L67YBbq6qjyVZnGT3qrp8Q3ZaVRey9i1EJu0/zbpF8/AlSVJPhn2s6ptoTmM9oW3aAvjEbBUlSZpbhh2zeBHwAuBWgKq6mvU/ZVaSNE8NGxZ3tYeDCqC9LkKSNCaGDYvTknwI2D7JK4Cv4oOQJGlsdA5wtzfy+zTwWOBm4DHAX1TV2bNcmyRpjugMi/Zq6rOq6vGAASFJY2jYw1AXJHnqrFYiSZqzhr3OYl/g95L8lOaMqNB0Op4wW4VJkuaOdYZFkkdU1X8DzxtRPZKkOairZ/EFmrvNXpHk9Kp68QhqkiTNMV1jFhmYfuRsFiJJmru6wqJmmJYkjZGuw1BPTHIzTQ9j63Ya7h/g3nZWq5MkzQnrDIuqWjSqQiRJc9ew11lIksaYYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnq1FtYJFmU5HtJ/qmd3z3JeUlWJvl0ki3b9q3a+ZXt8uV91SxJ46rPnsWfAJcOzL8TeE9V7QHcCBzTth8D3Ni2v6ddT5I0Qr2ERZJlwMHAR9r5AM8BPtuucgrwwnb6kHaedvn+7fqSpBHpq2fxXuDPgPva+R2An1fVPe38VcDSdnopcCVAu/ymdn1J0oiMPCyS/CZwXVWdv4nf99gkK5KsWLNmzaZ8a0kae330LJ4OvCDJT4FP0Rx+eh+wfZLJ52ssA1a106uAXQHa5dsB109906o6saomqmpi8eLFs/sdSNKYGXlYVNUJVbWsqpYDhwNfq6qXAf8KHNqudiRwRjt9ZjtPu/xrVeUjXiVphObSdRavB45LspJmTOKktv0kYIe2/Tjg+J7qk6Sx1fUM7llVVV8Hvt5O/wR42jTr3AG8ZKSFSZLWMpd6FpKkOcqwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVKnkYdFkl2T/GuSS5L8IMmftO0PS3J2kh+1Xx/atifJ+5OsTPL9JPuMumZJGnd99CzuAV5XVXsB+wGvSrIXcDxwTlXtCZzTzgMcCOzZvo4F/m70JUvSeBt5WFTVNVV1QTt9C3ApsBQ4BDilXe0U4IXt9CHAx6txLrB9kp1HW7UkjbdexyySLAeeDJwHLKmqa9pF1wJL2umlwJUDm13VtkmSRqS3sEjyK8DpwGuq6ubBZVVVQK3n+x2bZEWSFWvWrNmElUqSegmLJFvQBMUnq+pzbfPqycNL7dfr2vZVwK4Dmy9r29ZSVSdW1URVTSxevHj2ipekMdTH2VABTgIurap3Dyw6EziynT4SOGOg/Yj2rKj9gJsGDldJkkZg8x72+XTg94GLklzYtr0BeAdwWpJjgCuAw9plZwEHASuB24CjR1qtJGn0YVFV/w5khsX7T7N+Aa+a1aIkSevkFdySpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE7zJiySPD/JZUlWJjm+73okaZzMi7BIsgj4IHAgsBfw0iR79VuVJI2PeREWwNOAlVX1k6q6C/gUcEjPNUnS2Ni87wKGtBS4cmD+KmDfwRWSHAsc287+IsllI6ptI6XvAoaxI/CzvosYRjIvfp7zxjz5ac6Lz+c8+WzuNtOC+RIWnarqRODEvutYiJKsqKqJvuuQpuPnczTmy2GoVcCuA/PL2jZJ0gjMl7D4LrBnkt2TbAkcDpzZc02SNDbmxWGoqronyauBLwOLgI9W1Q96LmuceHhPc5mfzxFIVfVdgyRpjpsvh6EkST0yLCRJnQwLSfNKkpcM06ZNyzELTSvJVlV1Z1ebNGpJLqiqfbratGnNi7Oh1ItvA1N/+aZrk0YiyYHAQcDSJO8fWLQtcE8/VY0Pw0JrSbITze1Vtk7yZO6/48O2wDa9FSbB1cAK4AXA+QPttwCv7aWiMeJhKK0lyZHAUcAEzS/mpFuAk6vqc33UJU1KskVV3d13HePGsNC0kry4qk7vuw5pqiRPB95Mc9O7zWl6v1VVj+yzroXOsNC0kmwFvBhYzsDhyqp6a181SQBJfkhz2Ol84N7J9qq6vreixoBjFprJGcBNNL+QngGlueSmqvpS30WMG3sWmlaSi6tq777rkKZK8g6ae8R9joF/ZKrqgt6KGgP2LDSTbyV5fFVd1Hch0hSTDz4bfIZFAc/poZaxYc9C00pyCbAHcDnNf2+Tg4hP6LUwSb0wLDStJNM+XrGqrhh1LdKgJEuAtwO7VNWBSfYCfq2qTuq5tAXNe0NpWm0o7Ao8p52+DT8vmhtOpnm2zS7t/H8Br+mrmHHhL7+mleRNwOuBE9qmLYBP9FeR9Es7VtVpwH3QPByNgVNoNTsMC83kRTS3VbgVoKquBh7Sa0VS49YkO9AMapNkP5rTvDWLPBtKM7mrqirJ5C/kg/suSGodB5wJPCrJfwCLgUP7LWnhMyw0k9OSfAjYPskrgJcDH+65JomquiDJs4DH0Jyld5n3ipp9ng2lGSV5LnAAzS/kl6vq7J5LkkiyCDiYB96K5t191TQODAutU5JtWfsX8oYey5FIchZwB3AR7SA3QFW9pbeixoCHoTStJH8AvIXml/I+2ovyAO/sqb4t8+LQ0bNnoWkl+RHNhU4/67sWaVCSdwLnVNVX+q5lnNiz0Ex+THMhnjTXnAt8PslmwN3cfyuabfsta2GzZ6FptY9U/RhwHmvf2fOPeytKApJcDhwCXFT+ARsZexaayYeArzFlEFGaA64ELjYoRsuw0Ey2qKrj+i5CmsZPgK8n+RJr93o9dXYWGRaayZeSHAt8kbV/IT11Vn27vH1t2b40Ao5ZaFrtceGpqqo8dVYaQ4aFpHkhyRdpbx44nap6wQjLGTsehtJakvz2upZX1edGVYs0xV/3XcA4s2ehtST52DoWV1W9fGTFSJozDAtJ80KS06rqsCQXMc3hKG8BMrsMC60lye9V1SeSTHvarKcnqi9Jdq6qa3w+fD8cs9BUkw858ql4mlOq6pr2q6HQA3sWkuaV9jGqfwP8Ks11FouAW7031OyyZ6FpJXkQcAzwOOBBk+0OcGsO+ABwOPAZYAI4Anh0rxWNgc36LkBz1j8AOwHPA74BLANu6bUiqVVVK4FFVXVvVX0MeH7fNS109iw0kz2q6iVJDqmqU5L8I/BvfRclAbcl2RK4MMn/A67Bf3xnnT9gzeTu9uvPk+wNbAc8vMd6pEm/T/O369XArcCuwDovJtXGMyw0kxOTPBR4I3AmcAnwzn5LkgB4YVXdUVU3V9Vb2rsj/2bfRS10ng2laSXZvaou72qTRi3JBVW1z5S271XVk/uqaRw4ZqGZnA7sM6Xts8BTeqhFIslLgd8Fdk9y5sCihwDeOn+WGRZaS5LH0pwuu92Umwpuy8AptFIPvkUzmL0j8K6B9luA7/dS0RgxLDTVY2iO/24P/NZA+y3AK/ooSIJfXrl9BfBrfdcyjhyz0LSS/FpVfbvvOqSp2h7vO2nOzkv7Kq/gnl2GhaaVZDFNT2I5Az1Qr+BW35KsBH6rqi7tu5Zx4mEozeQMmovwvgrc23Mt0qDVBsXo2bPQtJJcWFVP6rsOaaok76O5Fc0XgDsn232K4+yyZ6GZ/FOSg6rqrL4LkabYFrgNOGCgrQDDYhbZs9C0ktwCbAPcRXPrDwcRpTHm7T40k+2Ao4C/bAPiccBze61IApI8Osk5SS5u55+Q5I1917XQGRaayQeB/YCXtvO30DxHQOrbh4ETaG92WVXfp3m+hWaRYxaayb5VtU+S7wFU1Y3tbaGlvm1TVd9JMth2T1/FjAt7FprJ3UkW0QwcTl53cV+/JUkA/CzJo7j/s3kozW1ANIvsWWgm7wc+Dzw8yduAQ2luVy717VXAicBjk6wCLgde1m9JC59nQ2lG7U0F96c5E+ocL4TSXJLkwcBmVeXjfkfAsJAkdXLMQpLUybCQNK8k2WqYNm1ahoWk+Wa6W+d7O/1Z5tlQkuaFJDsBS4GtkzyZ5sQLaO4VtU1vhY0Jw0LSfPE8mlvQLAPePdB+C/CGPgoaJ54NJWleSfLiqjq97zrGjWEhaV5Jsj3wF8Az26ZvAG+tqpt6K2oMOMAtab45iebQ02Ht62bgY71WNAbsWUiaV6Z7iqNPdpx99iwkzTe3J/n1yZkkTwdu77GesWDPQtK8kuSJwMdpHtAV4AbgqKr6z14LW+AMC0nzUpJtAarq5r5rGQeGhaR5pb21x4uB5QxcK1ZVb+2rpnHgRXmS5pszgJuA84E7e65lbNizkDSvJLm4qvbuu45x49lQkuabbyV5fN9FjBt7FpLmlSSXAHvQPE71TpozoqqqntBrYQucYSFpXkmy23TtVXXFqGsZJ4aFJKmTYxaSpE6GhSSpk2EhbaQkOyX5VJIfJzk/yVlJHp3k4r5rkzYVL8qTNkKSAJ8HTqmqw9u2JwJLei1M2sTsWUgb5zeAu6vq7ycb2hvaXTk5n2R5kn9LckH7+h9t+85JvpnkwiQXJ3lGkkVJTm7nL0ry2tF/S9ID2bOQNs7eNLedWJfrgOdW1R1J9gROBSaA3wW+XFVvS7II2AZ4ErB08grl9qlwUu8MC2n2bQF8IMmTgHuBR7ft3wU+mmQL4AtVdWGSnwCPTPI3wD8DX+mjYGkqD0NJG+cHwFM61nktsBp4Ik2PYkuAqvomzXOkVwEnJzmiqm5s1/s68ErgI7NTtrR+DAtp43wN2CrJsZMNSZ4A7DqwznbANVV1H/D7wKJ2vd2A1VX1YZpQ2CfJjsBmVXU68EZgn9F8G9K6eRhK2ghVVUleBLw3yeuBO4CfAq8ZWO1vgdOTHAH8C3Br2/5s4E+T3A38AjgCWAp8LMnkP3InzPb3IA3D231Ikjp5GEqS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUqf/D5XP15gLLG9YAAAAAElFTkSuQmCC"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset sample\n",
        "This is how a sample of SetFit/rte is organized\n",
        "* Label = 0, entailment.\n",
        "* Label = 1, not entailment."
      ],
      "metadata": {
        "id": "HsliOdmhsdGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Entailment : \"+ dataset[\"train\"][\"text1\"][0])\n",
        "print(\"\\n\")\n",
        "print(\"Contradiction : \"+ dataset[\"train\"][\"text2\"][0])\n",
        "print(\"\\n\")\n",
        "print(\"Label : \",dataset[\"train\"][\"label\"][0])\n",
        "print(\"\\n\")\n",
        "print(\"Text Label : \"+ dataset[\"train\"][\"label_text\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qry71iYfW892",
        "outputId": "31aeeb7a-8f00-4f67-bbb0-f523b2a904e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entailment : No Weapons of Mass Destruction Found in Iraq Yet.\n",
            "\n",
            "\n",
            "Contradiction : Weapons of Mass Destruction Found in Iraq.\n",
            "\n",
            "\n",
            "Label :  1\n",
            "\n",
            "\n",
            "Text Label : not entailment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in [\"text1\", \"text2\"] :\n",
        "  max_sentence_length = len(dataset[\"train\"][s][0])\n",
        "  for i in range(1,len(dataset[\"train\"])) :\n",
        "    if max_sentence_length < len(dataset[\"train\"][s][i]) :\n",
        "      max_sentence_length = len(dataset[\"train\"][s][i])\n",
        "  print(s + \" training sentece with max length : \", max_sentence_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32pfaB_mpLQC",
        "outputId": "ca91fba6-4032-479a-f8ee-4f5d66942d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text1 training sentece with max length :  1392\n",
            "text2 training sentece with max length :  230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model\n",
        "\n",
        "\n",
        "**DeBERTa-v3-base** is a version of the DeBERTa (Decoding-enhanced BERT with disentangled attention) model, which is a transformer-based language model developed by Microsoft.\n",
        "\n",
        "* **Disentangled Attention**: where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions.\n",
        "\n",
        "* **Enhanced Mask Decoder**:  is used to replace the output softmax layer to predict the masked tokens for model pretraining. DeBERTa introduces enhancements to the decoding process, allowing the model to generate more coherent and contextually relevant outputs during tasks such as text generation and language modeling.\n",
        "\n",
        "* **Robust Performance**: DeBERTa has demonstrated strong performance on various natural language processing tasks, including text classification, question answering, and language understanding tasks.\n",
        "\n",
        "* https://huggingface.co/docs/transformers/model_doc/deberta-v2"
      ],
      "metadata": {
        "id": "q14LSA6iynru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DeBERTa-v3-base tokenizer and model\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "#model_name = \"bert-base-cased\"\n",
        "deberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "deberta_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtYDY0coXnbu",
        "outputId": "53c90b3e-0503-40bd-c3ab-d704ab22a119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Info about the Tokenizer\n",
        "\n",
        "DebertaV2TokenizerFast is based on SentencePiece, which is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n",
        "\n",
        "* https://github.com/google/sentencepiece"
      ],
      "metadata": {
        "id": "xHzIwMEfjpM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(deberta_tokenizer).partition('\\n')[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFotrAjHhzi3",
        "outputId": "8dbfd838-df2a-479f-860b-d01dfedba326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-base', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\", '\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizzation\n",
        "\n",
        "In this phase, we would apply the tokenizer to each example belonging to the dataset SetFit/rte. As a result, the dataset will have additional features:\n",
        "\n",
        "\n",
        "1.  **input_ids**, convert the tokens into indices (IDs) associated with the DeBERTa vocabulary.\n",
        "\n",
        "2.   **token_type_ids**, identifies the belonging to a sentence in case there are two sentences in the same sample, <br> zeros belong to the first sentence and ones belong to the second sentence.\n",
        "\n",
        "3. **attention_mask**, is a binary tensor indicating the position of the padded indices so that the model does not attend to them. <br> (For the BertTokenizer, 1 indicates a value that should be attended to, while 0 indicates a padded value, look at this link for additional information :https://huggingface.co/docs/transformers/glossary#attention-mask )"
      ],
      "metadata": {
        "id": "u5uYK40anLlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling parallelism to avoid deadlocks\n",
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples, tokenizer):\n",
        "    return tokenizer(examples[\"text1\"], examples[\"text2\"], truncation = True, padding = 'max_length', max_length=256)"
      ],
      "metadata": {
        "id": "5TWBpwFhbUSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deberta_tokenized_data = dataset.map(lambda x: tokenize_function(x, deberta_tokenizer), batched=True) # map applies the tokenize_function to all the rows of the dataset.\n",
        "print(deberta_tokenized_data)\n",
        "# The new fields: input_ids, convert the tokens into indices (IDs) associated with the DeBERTa vocabulary.\n",
        "#                 token_type_ids, identifies the belonging to a sentence in case there are two sentences in the same sample\n",
        "#                 attention_mask, indicates to the model which tokens it should pay attention to during training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPBo8M8cbWx8",
        "outputId": "2a8e74d8-d1cb-404b-d4da-f1f04821554c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 2490\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 277\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of tokenized sentence\n",
        "For greater clarity, we will analyze how the DebertaV2TokenizerFast tokenizer  works <br> by extracting a training example from the Set/rte dataset, consisting of text1, text2, and label.<br>\n",
        "Notice that\n",
        "* Special tokens are the same as the primary version of the model BERT\n",
        "\n",
        "* Attention mask is zero only for padding\n",
        "\n",
        "* token_type_ids is set to one only for the sentence belonging to the feature text2."
      ],
      "metadata": {
        "id": "lhJnxyw6ySRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1) Example of first sentence  :\\n \", deberta_tokenized_data[\"train\"]['text1'][2])\n",
        "print(\"\\n\")\n",
        "print(\"2) Example of second sentence  :\\n \", deberta_tokenized_data[\"train\"]['text2'][2])\n",
        "print(\"\\n\")\n",
        "print(\"3) Example of RTE label :\\n \", deberta_tokenized_data[\"train\"]['label_text'][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0nOts41nFnm",
        "outputId": "7b5ca89b-533e-4380-eb30-00a0c7d0aab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Example of first sentence  :\n",
            "  Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\n",
            "\n",
            "\n",
            "2) Example of second sentence  :\n",
            "  Herceptin can be used to treat breast cancer.\n",
            "\n",
            "\n",
            "3) Example of RTE label :\n",
            "  entailment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the tokens ID of the first sentence in the input_ids field.\n",
        "first_example_tokens = deberta_tokenized_data[\"train\"][\"input_ids\"][2]\n",
        "\n",
        "# Convert tokens ID to tokens using the tokenizer\n",
        "tokens = [deberta_tokenizer.decode(token_id, skip_special_tokens=False) for token_id in first_example_tokens]\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVM13LtabeBX",
        "outputId": "55079173-792a-4c60-e450-4a6054743fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Her', 'cept', 'in', 'was', 'already', 'approved', 'to', 'treat', 'the', 'sick', 'est', 'breast', 'cancer', 'patients', ',', 'and', 'the', 'company', 'said', ',', 'Monday', ',', 'it', 'will', 'discuss', 'with', 'federal', 'regulators', 'the', 'possibility', 'of', 'prescribing', 'the', 'drug', 'for', 'more', 'breast', 'cancer', 'patients', '.', '[SEP]', 'Her', 'cept', 'in', 'can', 'be', 'used', 'to', 'treat', 'breast', 'cancer', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.Series(deberta_tokenized_data[\"train\"][\"attention_mask\"][2])\n",
        "b = pd.Series(deberta_tokenized_data[\"train\"][\"input_ids\"][2])\n",
        "c = pd.Series(deberta_tokenized_data[\"train\"][\"token_type_ids\"][2])\n",
        "\n",
        "df = pd.DataFrame({'tokens':tokens,'input_ids': b, 'attention_mask': a, 'token_type_ids':c})\n",
        "# Example of single sentence output after tokenization\n",
        "print(df[:60])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5LZtmkUkVDI",
        "outputId": "cc35066e-2686-41a3-d262-07a03cf4384e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         tokens  input_ids  attention_mask  token_type_ids\n",
            "0         [CLS]          1               1               0\n",
            "1           Her       1620               1               0\n",
            "2          cept      29813               1               0\n",
            "3            in        547               1               0\n",
            "4           was        284               1               0\n",
            "5       already        637               1               0\n",
            "6      approved       2664               1               0\n",
            "7            to        264               1               0\n",
            "8         treat       2435               1               0\n",
            "9           the        262               1               0\n",
            "10         sick       3808               1               0\n",
            "11          est       5260               1               0\n",
            "12       breast       4311               1               0\n",
            "13       cancer       1771               1               0\n",
            "14     patients       1171               1               0\n",
            "15            ,        261               1               0\n",
            "16          and        263               1               0\n",
            "17          the        262               1               0\n",
            "18      company        483               1               0\n",
            "19         said        357               1               0\n",
            "20            ,        261               1               0\n",
            "21       Monday       1420               1               0\n",
            "22            ,        261               1               0\n",
            "23           it        278               1               0\n",
            "24         will        296               1               0\n",
            "25      discuss       2065               1               0\n",
            "26         with        275               1               0\n",
            "27      federal       1819               1               0\n",
            "28   regulators      11522               1               0\n",
            "29          the        262               1               0\n",
            "30  possibility       3272               1               0\n",
            "31           of        265               1               0\n",
            "32  prescribing      27952               1               0\n",
            "33          the        262               1               0\n",
            "34         drug       1877               1               0\n",
            "35          for        270               1               0\n",
            "36         more        310               1               0\n",
            "37       breast       4311               1               0\n",
            "38       cancer       1771               1               0\n",
            "39     patients       1171               1               0\n",
            "40            .        260               1               0\n",
            "41        [SEP]          2               1               0\n",
            "42          Her       1620               1               1\n",
            "43         cept      29813               1               1\n",
            "44           in        547               1               1\n",
            "45          can        295               1               1\n",
            "46           be        282               1               1\n",
            "47         used        427               1               1\n",
            "48           to        264               1               1\n",
            "49        treat       2435               1               1\n",
            "50       breast       4311               1               1\n",
            "51       cancer       1771               1               1\n",
            "52            .        260               1               1\n",
            "53        [SEP]          2               1               1\n",
            "54        [PAD]          0               0               0\n",
            "55        [PAD]          0               0               0\n",
            "56        [PAD]          0               0               0\n",
            "57        [PAD]          0               0               0\n",
            "58        [PAD]          0               0               0\n",
            "59        [PAD]          0               0               0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define common metrics\n",
        "\n",
        "To evaluate the model, we could limit ourselves to assessing accuracy since we are dealing with a balanced dataset.<br> However, for a more comprehensive evaluation, we also include precision, recall, and f1_score."
      ],
      "metadata": {
        "id": "wTHwp_VWnCYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_prediction):\n",
        "    predictions, labels = eval_prediction\n",
        "    predictions = torch.from_numpy(predictions)  # Convert predictions to PyTorch tensor\n",
        "    labels = torch.from_numpy(labels)  # Convert labels to PyTorch tensor\n",
        "\n",
        "    predictions = torch.argmax(predictions, dim=1).cpu().numpy()  # Convert to NumPy array\n",
        "    labels = labels.cpu().numpy()  # Convert to NumPy array\n",
        "\n",
        "    accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
        "    precision = sklearn.metrics.precision_score(labels, predictions, average='binary')\n",
        "    recall = sklearn.metrics.recall_score(labels, predictions, average='binary')\n",
        "    f1 = sklearn.metrics.f1_score(labels, predictions, average='binary')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }"
      ],
      "metadata": {
        "id": "XIB00WxMziBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if GPU is available"
      ],
      "metadata": {
        "id": "nflBBGwMt0KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    print(f\"Training on GPU: {torch.cuda.get_device_name(device)}\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5muOAFI4qb8",
        "outputId": "03390838-7eb7-4a57-ff55-99083ed3befe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU: NVIDIA GeForce RTX 3060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized Search\n",
        "\n",
        "* In this section, we want to create a 'study' using the Optuna library which is useful for hyperparameter search. <br> First, we need to define a function to optimize. In our case, during the fine-tuning of the model we have chosen, we want it to perform well on the validation set. Therefore, we will maximize the accuracy on the validation set.\n",
        "\n",
        "* The choice of hyperparameters is crucial for achieving good learning results and also for avoiding high computational costs. <br>In particular, a batch size of 8 speeds up learning by using 8 examples to evaluate one gradient descent step. <br> Setting evaluation steps to 100 increases the frequency of updating the model's performance on the validation set. <br> A learning rate between 10^(-6) and 10^(-4) should allow us to reach convergence a bit more slowly but with greater certainty. <br> Weight decay between 10^(-5) and 10^(-3) should significantly reduce the size of the weight matrix, acting as a regularizer. Squared L2 norm should aid the fine-tuning process by lowering accuracy on the training set and reducing overfitting.\n",
        "\n",
        "* The hyperparameter selection is random only for the learning rate and weight decay, as they can take values of vastly different scales, and it's not straightforward to determine the best possible value for optimal performance. The choice of such a value follows a log-uniform distribution.\n",
        "(https://en.wikipedia.org/wiki/Reciprocal_distribution)\n",
        "\n",
        "* Additional information about how optuna works :\n",
        "* https://medium.com/carbon-consulting/transformer-models-hyperparameter-optimization-with-the-optuna-299e185044a8\n",
        "* https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html\n",
        "* https://optuna.readthedocs.io/en/stable/faq.html#what-happens-when-i-dynamically-alter-a-search-space"
      ],
      "metadata": {
        "id": "33YISeIiIwI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optuna_hp_space(trial):\n",
        "  return {\n",
        "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", low = 5e-6, high = 1e-4),\n",
        "        #\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", low = 2, high = 5),\n",
        "        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3), # applying the norm L2^2, reduce weights dimension, reduce overfitting\n",
        "    }\n",
        "\n",
        "# trainer.hyperparameter_search(direction=\"maximize\", hp_space = optuna_hp_space)"
      ],
      "metadata": {
        "id": "qz_zGpvFIlFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing Hyperparameters with Optuna: From Experiments to Fine-Tuning\n",
        "\n",
        "We define the hyperparameter grid as previously specified and define the objective function to optimize,<br> which is the accuracy on the validation set. Next, we will conduct several experiments (n_trials), <br>select the one that satisfies us the most in terms of performance, and then proceed to fine-tune it."
      ],
      "metadata": {
        "id": "rPWOyG6quH_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHB6xPuyJw0Q"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  # Define hyperparameters to search over\n",
        "  hparams = optuna_hp_space(trial)\n",
        "\n",
        "  # Set up training arguments with current hyperparameters\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=f\"./DeBERTa_rte_steps_100_batch_8/trial_{trial.number}\",\n",
        "      evaluation_strategy=\"steps\", #The evaluation strategy to adopt during training, \"epoch\" or \"steps\": Evaluation is done (and logged) every eval_steps or epoch\n",
        "      eval_steps= 100,# eval_steps is a parameter that specifies how often the model should evaluate its performance on the validation dataset during training, measured in terms of training steps\n",
        "      per_device_train_batch_size =  8, # Define the batch size for training on each device (e.g., GPU).\n",
        "      learning_rate = hparams[\"learning_rate\"],\n",
        "      weight_decay = hparams[\"weight_decay\"],\n",
        "      num_train_epochs =  10, # num_train_epochs is a parameter that specifies the number of epochs (complete passes through the entire training dataset) to train the model.\n",
        "      logging_steps = 100,\n",
        "      logging_dir = f\"./DeBERTa_rte_steps_100_batch_8/trial_{trial.number}\",\n",
        "      overwrite_output_dir = True,\n",
        "      disable_tqdm = False,  # Enable tqdm progress bars\n",
        "  )\n",
        "  # add callbacks to stop training if the maximization of the objective function gives bad result after \"patience\" epochs\n",
        "  callbacks = [EarlyStoppingCallback(early_stopping_patience=7)]\n",
        "\n",
        "  # Initialize the Trainer\n",
        "  trainer = Trainer(\n",
        "    model=deberta_model,\n",
        "    args=training_args,\n",
        "    train_dataset=deberta_tokenized_data[\"train\"],\n",
        "    eval_dataset=deberta_tokenized_data[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "  # Record the starting time\n",
        "  # start_time = time.time()\n",
        "\n",
        "  # Fine-tune the model with Cross-Entropy loss\n",
        "  trainer.train()\n",
        "\n",
        "  # Evaluate the model and return the validation metric\n",
        "  eval_results = trainer.evaluate()\n",
        "\n",
        "  return eval_results[\"eval_accuracy\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results of DeBERTa Model Experiments\n",
        "\n",
        "Considering both the trend of the loss and the accuracy on the validation set, <br> with a broader look at the problem, trial 0 is the one that satisfies us the most.\n"
      ],
      "metadata": {
        "id": "0nUXYEYrOhjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform hyperparameter search with Optuna\n",
        "start_time = time.time()\n",
        "study = optuna.create_study(study_name='hyper-parameter-search', direction=\"maximize\", sampler=TPESampler(seed=42))\n",
        "study.optimize(objective, n_trials = 5)\n",
        "end_time = time.time()\n",
        "study_time = end_time - start_time\n",
        "\n",
        "# Gives the best validation accuracy value\n",
        "print(\"study best value is : \\n\")\n",
        "print(study.best_value)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Gives the best hyperparameter values to get the best validation accuracy value\n",
        "print(\"study best parameters are : \\n\")\n",
        "print(study.best_params)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Return info about best Trial such as start and end datetime, hyperparameters\n",
        "print(\"study best trial is : \\n\")\n",
        "print(study.best_trial)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# study time\n",
        "print(\"time to complete study : \", study_time)\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kGH0zrbCQOFw",
        "outputId": "82cc4d38-503a-458f-9b8d-65708d3f3ad5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-25 12:33:42,188] A new study created in memory with name: hyper-parameter-search\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 21:20, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.695800</td>\n",
              "      <td>0.682630</td>\n",
              "      <td>0.527076</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.662600</td>\n",
              "      <td>0.551113</td>\n",
              "      <td>0.743682</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.687023</td>\n",
              "      <td>0.717131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.546400</td>\n",
              "      <td>0.508267</td>\n",
              "      <td>0.776173</td>\n",
              "      <td>0.855670</td>\n",
              "      <td>0.633588</td>\n",
              "      <td>0.728070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.423100</td>\n",
              "      <td>0.529771</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.793388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.387900</td>\n",
              "      <td>0.458151</td>\n",
              "      <td>0.851986</td>\n",
              "      <td>0.835821</td>\n",
              "      <td>0.854962</td>\n",
              "      <td>0.845283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>0.506453</td>\n",
              "      <td>0.844765</td>\n",
              "      <td>0.838462</td>\n",
              "      <td>0.832061</td>\n",
              "      <td>0.835249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.244700</td>\n",
              "      <td>0.712960</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.897196</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.806723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.747608</td>\n",
              "      <td>0.855596</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.843750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.197600</td>\n",
              "      <td>0.952183</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.860870</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.804878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.167200</td>\n",
              "      <td>1.020486</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.817121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.126800</td>\n",
              "      <td>1.110988</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.858407</td>\n",
              "      <td>0.740458</td>\n",
              "      <td>0.795082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.055100</td>\n",
              "      <td>1.126859</td>\n",
              "      <td>0.837545</td>\n",
              "      <td>0.864407</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.819277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.057300</td>\n",
              "      <td>1.168197</td>\n",
              "      <td>0.844765</td>\n",
              "      <td>0.885965</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.824490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.044800</td>\n",
              "      <td>1.126227</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.848000</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.828125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.062600</td>\n",
              "      <td>1.080102</td>\n",
              "      <td>0.851986</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.842912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.076500</td>\n",
              "      <td>1.134307</td>\n",
              "      <td>0.848375</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.839695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>1.175550</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.830769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.025400</td>\n",
              "      <td>1.232810</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.816000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.033400</td>\n",
              "      <td>1.235411</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.824427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>1.242227</td>\n",
              "      <td>0.837545</td>\n",
              "      <td>0.858333</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.820717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.010200</td>\n",
              "      <td>1.333430</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.810811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.019900</td>\n",
              "      <td>1.402823</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.815686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>1.413845</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.848739</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.808000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>1.414973</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.812749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>1.424284</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.855932</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.811245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>1.429503</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.867257</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.803279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>1.450862</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.817460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.016700</td>\n",
              "      <td>1.477418</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.844262</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.814229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.495168</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.837398</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.811024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>1.477021</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.812749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.015000</td>\n",
              "      <td>1.489921</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.844262</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.814229</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-25 12:55:07,099] Trial 0 finished with value: 0.8267148014440433 and parameters: {'learning_rate': 1.5355286838886852e-05, 'weight_decay': 0.0007969454818643932}. Best is trial 0 with value: 0.8267148014440433.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 21:06, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.247800</td>\n",
              "      <td>1.080141</td>\n",
              "      <td>0.794224</td>\n",
              "      <td>0.768116</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.788104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.240600</td>\n",
              "      <td>1.999705</td>\n",
              "      <td>0.722022</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.465649</td>\n",
              "      <td>0.613065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.199700</td>\n",
              "      <td>1.506592</td>\n",
              "      <td>0.761733</td>\n",
              "      <td>0.809524</td>\n",
              "      <td>0.648855</td>\n",
              "      <td>0.720339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.176200</td>\n",
              "      <td>1.306150</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.803089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.204700</td>\n",
              "      <td>1.328071</td>\n",
              "      <td>0.779783</td>\n",
              "      <td>0.710843</td>\n",
              "      <td>0.900763</td>\n",
              "      <td>0.794613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.155800</td>\n",
              "      <td>1.038432</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.148400</td>\n",
              "      <td>0.956365</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.773973</td>\n",
              "      <td>0.862595</td>\n",
              "      <td>0.815884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.159100</td>\n",
              "      <td>1.011042</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.803030</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.806084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.145800</td>\n",
              "      <td>1.899058</td>\n",
              "      <td>0.736462</td>\n",
              "      <td>0.881579</td>\n",
              "      <td>0.511450</td>\n",
              "      <td>0.647343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.165700</td>\n",
              "      <td>1.294752</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.834783</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.780488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.042900</td>\n",
              "      <td>1.383219</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.796935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.068700</td>\n",
              "      <td>1.335544</td>\n",
              "      <td>0.797834</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.725191</td>\n",
              "      <td>0.772358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.056800</td>\n",
              "      <td>1.517485</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.843478</td>\n",
              "      <td>0.740458</td>\n",
              "      <td>0.788618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>1.210921</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.816794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.072100</td>\n",
              "      <td>1.388849</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.754839</td>\n",
              "      <td>0.893130</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.087600</td>\n",
              "      <td>1.394706</td>\n",
              "      <td>0.790614</td>\n",
              "      <td>0.744966</td>\n",
              "      <td>0.847328</td>\n",
              "      <td>0.792857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.072800</td>\n",
              "      <td>1.151306</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.791367</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.814815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.038200</td>\n",
              "      <td>1.437381</td>\n",
              "      <td>0.801444</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.790875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.013700</td>\n",
              "      <td>1.466648</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.816794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.017700</td>\n",
              "      <td>1.548568</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.827869</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.798419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>1.473021</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.824000</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.804688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>1.492876</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.824427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.620052</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.824000</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.804688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>1.699477</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.834711</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.801587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.035200</td>\n",
              "      <td>1.636195</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.818533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.015600</td>\n",
              "      <td>1.664990</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.810811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.821603</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.788845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>1.655843</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.803150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>1.803357</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.806019</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.810308</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-25 13:16:17,423] Trial 1 finished with value: 0.8050541516245487 and parameters: {'learning_rate': 4.4803926826840614e-05, 'weight_decay': 0.00015751320499779721}. Best is trial 0 with value: 0.8267148014440433.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 20:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.691207</td>\n",
              "      <td>0.787004</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.687023</td>\n",
              "      <td>0.753138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.662573</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.783673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.762963</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.834783</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.780488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.067825</td>\n",
              "      <td>0.787004</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.687023</td>\n",
              "      <td>0.753138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>2.771522</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.751634</td>\n",
              "      <td>0.877863</td>\n",
              "      <td>0.809859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.045100</td>\n",
              "      <td>2.200373</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.834646</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.821705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.013300</td>\n",
              "      <td>2.137686</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.017700</td>\n",
              "      <td>2.206198</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.836066</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.806324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.157055</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.834646</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.821705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>2.029636</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.826772</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.813953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.075812</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.121505</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.844262</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.814229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.506887</td>\n",
              "      <td>0.790614</td>\n",
              "      <td>0.841121</td>\n",
              "      <td>0.687023</td>\n",
              "      <td>0.756303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.373900</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.798387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>2.422184</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.854545</td>\n",
              "      <td>0.717557</td>\n",
              "      <td>0.780083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.409399</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.783673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.022800</td>\n",
              "      <td>2.239378</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.840336</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.211596</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.817121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.234037</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.242634</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.251170</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.259246</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.266257</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>2.202119</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.815385</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.812261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.245561</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.817121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.245507</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.247564</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.249340</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.250732</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.021600</td>\n",
              "      <td>2.244710</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.817121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.243714</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.817121</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-25 13:36:22,720] Trial 2 finished with value: 0.8303249097472925 and parameters: {'learning_rate': 7.979118876474868e-06, 'weight_decay': 2.0511104188433963e-05}. Best is trial 2 with value: 0.8303249097472925.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 19:23, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.693255</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.848739</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.808000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.920522</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.798387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.968400</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.841667</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.804781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.087546</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.796935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.138637</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.796935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.282934</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.305600</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.323455</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.340673</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.356057</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.360574</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.373510</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.384701</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.396478</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.407484</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.420431</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.429689</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.437809</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.433027</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.438642</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.445518</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.451727</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.456972</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.460725</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.464869</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.468940</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.471614</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.474186</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.475936</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.467690</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.019200</td>\n",
              "      <td>3.468902</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-25 13:55:49,649] Trial 3 finished with value: 0.8158844765342961 and parameters: {'learning_rate': 5.950295391592121e-06, 'weight_decay': 0.0005399484409787432}. Best is trial 2 with value: 0.8303249097472925.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 19:21, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.622409</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.715915</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.776462</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.821442</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.857996</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.889584</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.916267</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.938492</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.957604</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.974280</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.988750</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.002919</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.015080</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.026756</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.037086</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.047432</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.055531</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.062647</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.069863</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.076249</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.081573</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.086454</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.090678</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.094360</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.097521</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.100594</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.102687</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.104431</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.105668</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.106230</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.796813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>4.126881</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.831933</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-25 14:15:15,088] Trial 4 finished with value: 0.8122743682310469 and parameters: {'learning_rate': 3.027182927734626e-05, 'weight_decay': 0.0002607024758370766}. Best is trial 2 with value: 0.8303249097472925.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study best value is : \n",
            "\n",
            "0.8303249097472925\n",
            "\n",
            "\n",
            "\n",
            "study best parameters are : \n",
            "\n",
            "{'learning_rate': 7.979118876474868e-06, 'weight_decay': 2.0511104188433963e-05}\n",
            "\n",
            "\n",
            "\n",
            "study best trial is : \n",
            "\n",
            "FrozenTrial(number=2, state=TrialState.COMPLETE, values=[0.8303249097472925], datetime_start=datetime.datetime(2024, 6, 25, 13, 16, 17, 423416), datetime_complete=datetime.datetime(2024, 6, 25, 13, 36, 22, 720679), params={'learning_rate': 7.979118876474868e-06, 'weight_decay': 2.0511104188433963e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.0001, log=True, low=5e-06, step=None), 'weight_decay': FloatDistribution(high=0.001, log=True, low=1e-05, step=None)}, trial_id=2, value=None)\n",
            "\n",
            "\n",
            "\n",
            "time to complete study :  6092.901935338974\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best accuracy found\n",
        "study.best_trial.values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGYn9p2ofzvg",
        "outputId": "05f60674-6259-409e-bf25-e21a152ba494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8303249097472925"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# study with best validation accuracy\n",
        "study.best_trial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z__G5roY2vPo",
        "outputId": "7ee26c19-4c95-421f-eba0-dfbe5a1cc59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenTrial(number=2, state=TrialState.COMPLETE, values=[0.8303249097472925], datetime_start=datetime.datetime(2024, 6, 25, 13, 16, 17, 423416), datetime_complete=datetime.datetime(2024, 6, 25, 13, 36, 22, 720679), params={'learning_rate': 7.979118876474868e-06, 'weight_decay': 2.0511104188433963e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.0001, log=True, low=5e-06, step=None), 'weight_decay': FloatDistribution(high=0.001, log=True, low=1e-05, step=None)}, trial_id=2, value=None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# study with the lowest validation loss\n",
        "study.trials[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7Lijnyd1awh",
        "outputId": "0695930b-3499-4ee2-c01d-24c222875fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[0.8267148014440433], datetime_start=datetime.datetime(2024, 6, 25, 12, 33, 42, 189932), datetime_complete=datetime.datetime(2024, 6, 25, 12, 55, 7, 99450), params={'learning_rate': 1.5355286838886852e-05, 'weight_decay': 0.0007969454818643932}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.0001, log=True, low=5e-06, step=None), 'weight_decay': FloatDistribution(high=0.001, log=True, low=1e-05, step=None)}, trial_id=0, value=None)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.trials[0].params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgXmtt5Iy31q",
        "outputId": "ca7cad71-4c34-4557-aae1-9aeb3dea1db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 1.5355286838886852e-05,\n",
              " 'weight_decay': 0.0007969454818643932}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy study.trials[0] parameters in case of out_of_memory problem\n",
        "params = {'learning_rate': 1.5355286838886852e-05,\n",
        " 'weight_decay': 0.0007969454818643932}"
      ],
      "metadata": {
        "id": "j8tLlXXC5j2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeBERTa Inference on test set\n",
        "\n",
        "We have selected a **small** subset of the test set to be able to compare the models on unseen data. <br>\n",
        "To do this, we will ask Chat-GPT to provide **labels** on unseen data, <br> then we will compare the predictions between the various models before and after performing fine-tuning <br> (on the SetFit/rte dataset)."
      ],
      "metadata": {
        "id": "3sgTdGae164X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# provides a subset of the test set fixed\n",
        "def get_test_data (dataset):\n",
        "  test_data = dataset[\"test\"][200:220]\n",
        "  test_dataset = Dataset.from_dict(test_data)\n",
        "  test_dataset = test_dataset.remove_columns(['label', 'label_text'])\n",
        "  return test_dataset"
      ],
      "metadata": {
        "id": "ewp_qe6YqnlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = get_test_data(deberta_tokenized_data)"
      ],
      "metadata": {
        "id": "2pQ7Vc-Bjbyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['text1']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpx4NWP4otvq",
        "outputId": "04bb2f45-09d3-4adc-8877-e8cc19d77b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Scientists have discovered that drinking tea protects against heart disease by improving the function of the artery walls.',\n",
              " \"Most notably, the Women's Health Initiative recently found that postmenopausal women taking estrogen with progestin have an increased risk of breast cancer as well as heart attack, stroke, and blood clots.\",\n",
              " 'Although panic disorders and agoraphobia are widespread in the general population, almost half of those afflicted with these problems do not receive treatment for them.',\n",
              " 'Restricted items are those items that are not acceptable for purchase using government appropriated funds. Some examples of items that the General Accounting Office (GAO) considers \"restricted\" include: food, live entertainment, and personal gifts including plaques.',\n",
              " \"Folk art embodies different parts of the nation's indigenous culture, minority languages, and philosophy.\",\n",
              " 'Since August 1999 an increase of the seismic activity has marked the beginning of a seismic sequence which lasted until November 1999, even if smaller earthquakes were recorded until January 2000.',\n",
              " 'During the report period, there were no significant changes in seismicity or ground deformation.',\n",
              " 'Hundreds of divers and treasure hunters, including the Duke of Argyll, have risked their lives in the dangerous waters of the Isle of Mull trying to discover the reputed 30,000,000 pounds in Gold carried by this vessel--the target of the most enduring treasure hunt in British history.',\n",
              " '\"This industrial spills problem has persisted for years and it is about time that government took decisive action,\" says Paul Muldoon, Executive Director of the Canadian Environmental Law Association.',\n",
              " 'Canadian Nation Defense has been using virtual reality to train pilots and ground soldiers.',\n",
              " 'A shark encounter with a human typically consists of a shark leisurely circling and/or slowly swimming past the subject without any aggressive behavior being exhibited.',\n",
              " 'Although many herbs have been grown for centuries for medicinal use, do not look to herbal teas to remedy illness or disease.',\n",
              " 'Tea also contains anti-oxidants that help to reduce the chances of heart disease and cancer.',\n",
              " 'Agoraphobia means fear of open spaces and is one of the most common phobias.',\n",
              " \"Japan's economy is definitely not booming but there is a sign of recovery as the export is increasing. However, the decrease of motor vehicle export to North America shows caution.\",\n",
              " 'A great earthquake occurred at 19:50:08 (UTC) on Thursday, September 25, 2003. The magnitude 8.0 event has been located in the Hokkaido, Japan region.',\n",
              " 'The rusted pipelines have ruptured, spilling 2.5 million barrels of oil over the land and waterways of the Niger Delta.',\n",
              " 'China and Japan battle over the proposed route of a Russian oil pipeline from Siberia that will determine which nation functions and which does not.',\n",
              " 'An explosion at a historic synagogue in Tunisia left 21 dead, including 14 German tourists.',\n",
              " 'Government subsidies for energy and raw materials held prices at artificially low levels, doing away with incentives to conserve natural resources.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['text2']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PVp9RylpFNW",
        "outputId": "2566e736-67fc-4c77-cd2d-6c2cebed9a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tea protects from some diseases.',\n",
              " 'Estrogen is used by postmenopausal women.',\n",
              " 'Agoraphobia is a widespread disorder.',\n",
              " 'A government purchases food.',\n",
              " 'Indigenous folk art is preserved.',\n",
              " 'There was an increase in seismic activity.',\n",
              " 'There was an increase in seismic activity',\n",
              " 'Shipwreck salvaging was attempted.',\n",
              " 'The government took measures to reduce pollution.',\n",
              " 'Soldiers have been trained using virtual reality.',\n",
              " 'A shark attacked a human being.',\n",
              " 'Tea protects from illness.',\n",
              " 'Tea protects from some diseases.',\n",
              " 'Agoraphobia is a widespread disorder.',\n",
              " 'Cars exported by Japan decreased.',\n",
              " 'An earthquake occurred in Hokkaido, Japan.',\n",
              " 'An oil pipeline in Siberia was ruptured.',\n",
              " 'An oil pipeline in Siberia was ruptured.',\n",
              " 'A European synagogue was the target of an act of terrorism.',\n",
              " 'The government took measures to protect the environment.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGpt RTE predictions on test set\n",
        "\n",
        "1. 'Scientists have discovered that drinking tea protects against heart disease by improving the function of the artery walls.' - 'Tea protects from some diseases.' → 0\n",
        "2. \"Most notably, the Women's Health Initiative recently found that postmenopausal women taking estrogen with progestin have an increased risk of breast cancer as well as heart attack, stroke, and blood clots.\" - 'Estrogen is used by postmenopausal women.' → 0\n",
        "3. 'Although panic disorders and agoraphobia are widespread in the general population, almost half of those afflicted with these problems do not receive treatment for them.' - 'Agoraphobia is a widespread disorder.' → 0\n",
        "4. 'Restricted items are those items that are not acceptable for purchase using government appropriated funds. Some examples of items that the General Accounting Office (GAO) considers \"restricted\" include: food, live entertainment, and personal gifts including plaques.' - 'A government purchases food.' → 1\n",
        "5. \"Folk art embodies different parts of the nation's indigenous culture, minority languages, and philosophy.\" - 'Indigenous folk art is preserved.' → 1\n",
        "6. 'Since August 1999 an increase of the seismic activity has marked the beginning of a seismic sequence which lasted until November 1999, even if smaller earthquakes were recorded until January 2000.' - 'There was an increase in seismic activity.' → 0\n",
        "7. 'During the report period, there were no significant changes in seismicity or ground deformation.' - 'There was an increase in seismic activity.' → 1\n",
        "8. 'Hundreds of divers and treasure hunters, including the Duke of Argyll, have risked their lives in the dangerous waters of the Isle of Mull trying to discover the reputed 30,000,000 pounds in Gold carried by this vessel--the target of the most enduring treasure hunt in British history.' - 'Shipwreck salvaging was attempted.' → 0\n",
        "9. '\"This industrial spills problem has persisted for years and it is about time that government took decisive action,\" says Paul Muldoon, Executive Director of the Canadian Environmental Law Association.' - 'The government took measures to reduce pollution.' → 1\n",
        "10. 'Canadian Nation Defense has been using virtual reality to train pilots and ground soldiers.' - 'Soldiers have been trained using virtual reality.' → 0\n",
        "11. 'A shark encounter with a human typically consists of a shark leisurely circling and/or slowly swimming past the subject without any aggressive behavior being exhibited.' - 'A shark attacked a human being.' → 1\n",
        "12. 'Although many herbs have been grown for centuries for medicinal use, do not look to herbal teas to remedy illness or disease.' - 'Tea protects from illness.' → 1\n",
        "13. 'Tea also contains anti-oxidants that help to reduce the chances of heart disease and cancer.' - 'Tea protects from some diseases.' → 0\n",
        "14. 'Agoraphobia means fear of open spaces and is one of the most common phobias.' - 'Agoraphobia is a widespread disorder.' → 0\n",
        "15. \"Japan's economy is definitely not booming but there is a sign of recovery as the export is increasing. However, the decrease of motor vehicle export to North America shows caution.\" - 'Cars exported by Japan decreased.' → 0\n",
        "16. 'A great earthquake occurred at 19:50:08 (UTC) on Thursday, September 25, 2003. The magnitude 8.0 event has been located in the Hokkaido, Japan region.' - 'An earthquake occurred in Hokkaido, Japan.' → 0\n",
        "17. 'The rusted pipelines have ruptured, spilling 2.5 million barrels of oil over the land and waterways of the Niger Delta.' - 'An oil pipeline in Siberia was ruptured.' → 1\n",
        "18. 'China and Japan battle over the proposed route of a Russian oil pipeline from Siberia that will determine which nation functions and which does not.' - 'An oil pipeline in Siberia was ruptured.' → 1\n",
        "19. 'An explosion at a historic synagogue in Tunisia left 21 dead, including 14 German tourists.' - 'A European synagogue was the target of an act of terrorism.' → 1\n",
        "20. 'Government subsidies for energy and raw materials held prices at artificially low levels, doing away with incentives to conserve natural resources.' - 'The government took measures to protect the environment.' → 1"
      ],
      "metadata": {
        "id": "wWDC-gJSyRam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat-Gpt labels on test set\n",
        "gpt_labels = [0,0,0,1,1,0,1,0,1,0,1,1,0,0,0,0,1,1,1,1]"
      ],
      "metadata": {
        "id": "xGKr8OL8Tr1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeBERTa before fine-tuning\n",
        "\n",
        "At this stage, we will use the newly loaded DeBERTa v3-base model to perform inference on the test set.<br> We will then compare its predictions with those of ChatGPT."
      ],
      "metadata": {
        "id": "UthH0U7My5UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deberta_base_predictions = []\n",
        "\n",
        "# reload DeBERTa v3 base\n",
        "deberta_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# set the model in evaluation mode\n",
        "deberta_model.eval()\n",
        "\n",
        "# generate predictions on test set\n",
        "with torch.no_grad():\n",
        "    for example in test_dataset:\n",
        "        input_ids = torch.tensor([example['input_ids']])#.to(device)\n",
        "        attention_mask = torch.tensor([example['attention_mask']])#.to(device)\n",
        "\n",
        "        outputs = deberta_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1).cpu().item()\n",
        "        deberta_base_predictions.append(predictions)\n",
        "\n",
        "print(\"DeBERTa v3 base predictions: \",deberta_base_predictions)\n",
        "print(\"f1-score : \",sklearn.metrics.f1_score(gpt_labels, deberta_base_predictions,  average='binary'))\n",
        "print(\"precision-score : \",sklearn.metrics.precision_score(gpt_labels, deberta_base_predictions,  average='binary'))\n",
        "print(\"recall-score : \",sklearn.metrics.recall_score(gpt_labels, deberta_base_predictions,  average='binary'))\n",
        "print(\"accuracy-score : \",sklearn.metrics.accuracy_score(gpt_labels, deberta_base_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ojzaUS8pgeY",
        "outputId": "fc971ff5-0698-400f-9466-1ea3f5f1fa05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa v3 base predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "f1-score :  0.6666666666666666\n",
            "precision-score :  0.5\n",
            "recall-score :  1.0\n",
            "accuracy-score :  0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeBERTa after fine-tuning\n",
        "\n",
        "Now we will proceed with fine-tuning, which means we will take the DeBERTa v3-base model <br> and refine it by training it on our labeled SetFit/rte dataset so that it becomes more adept at performing the RTE task. <br> Then we will compare its predictions with those of ChatGPT."
      ],
      "metadata": {
        "id": "JHM4-IL8pLEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments with current hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    evaluation_strategy=\"steps\", #The evaluation strategy to adopt during training, \"epoch\" or \"steps\": Evaluation is done (and logged) every eval_steps or epoch\n",
        "    output_dir=f\"./DeBERTa_rte_steps_100_batch_8/best_param/\",\n",
        "    eval_steps= 100,# eval_steps is a parameter that specifies how often the model should evaluate its performance on the validation dataset during training, measured in terms of training steps\n",
        "    per_device_train_batch_size =  8, # Define the batch size for training on each device (e.g., GPU).\n",
        "    learning_rate =  params['learning_rate'],\n",
        "    weight_decay =  params['weight_decay'],\n",
        "    num_train_epochs =  10, # num_train_epochs is a parameter that specifies the number of epochs (complete passes through the entire training dataset) to train the model.\n",
        "    logging_steps = 100,\n",
        "    overwrite_output_dir = True,\n",
        "    disable_tqdm = False,  # Enable tqdm progress bars\n",
        ")\n",
        "# add callbacks to stop training if the maximization of the objective function gives bad result after \"patience\" epochs\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=7)]\n",
        "\n",
        "# Initialize the Trainer\n",
        "deberta_fine_tuned = Trainer(\n",
        "  model=deberta_model,\n",
        "  args=training_args,\n",
        "  train_dataset=deberta_tokenized_data[\"train\"],\n",
        "  eval_dataset=deberta_tokenized_data[\"validation\"],\n",
        "  compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Fine-tune the model with Cross-Entropy loss\n",
        "deberta_fine_tuned.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2GUy7A0hpKFD",
        "outputId": "3d815aa6-54c4-4c4f-df65-6adaf676d9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 20:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.688400</td>\n",
              "      <td>0.662390</td>\n",
              "      <td>0.606498</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>0.229008</td>\n",
              "      <td>0.355030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.668800</td>\n",
              "      <td>0.575116</td>\n",
              "      <td>0.740072</td>\n",
              "      <td>0.739837</td>\n",
              "      <td>0.694656</td>\n",
              "      <td>0.716535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.566589</td>\n",
              "      <td>0.725632</td>\n",
              "      <td>0.876712</td>\n",
              "      <td>0.488550</td>\n",
              "      <td>0.627451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.503500</td>\n",
              "      <td>0.477175</td>\n",
              "      <td>0.787004</td>\n",
              "      <td>0.867347</td>\n",
              "      <td>0.648855</td>\n",
              "      <td>0.742358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.424800</td>\n",
              "      <td>0.462045</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.795455</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.798479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>0.459594</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.854962</td>\n",
              "      <td>0.826568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.285400</td>\n",
              "      <td>0.700236</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.872727</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.796680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.225800</td>\n",
              "      <td>0.901916</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.854545</td>\n",
              "      <td>0.717557</td>\n",
              "      <td>0.780083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.299100</td>\n",
              "      <td>0.900132</td>\n",
              "      <td>0.801444</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.671756</td>\n",
              "      <td>0.761905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.220200</td>\n",
              "      <td>0.856887</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.845528</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.818898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.198400</td>\n",
              "      <td>1.040383</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.871560</td>\n",
              "      <td>0.725191</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.184500</td>\n",
              "      <td>0.986762</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.871560</td>\n",
              "      <td>0.725191</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.088100</td>\n",
              "      <td>1.052656</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.732824</td>\n",
              "      <td>0.790123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.104300</td>\n",
              "      <td>1.104947</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.818898</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.806202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.108500</td>\n",
              "      <td>1.030734</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.807843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.053800</td>\n",
              "      <td>1.092752</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.817121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.049800</td>\n",
              "      <td>1.158575</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.860870</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.804878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.055400</td>\n",
              "      <td>1.172548</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.848739</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.808000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.059800</td>\n",
              "      <td>1.218980</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.859649</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>1.235980</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.815686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.008600</td>\n",
              "      <td>1.298951</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.022200</td>\n",
              "      <td>1.331126</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.837398</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.811024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.054500</td>\n",
              "      <td>1.311302</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.876106</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.811475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>1.300605</td>\n",
              "      <td>0.833935</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.813008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.047800</td>\n",
              "      <td>1.240126</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.871795</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.822581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.029000</td>\n",
              "      <td>1.232339</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.826772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.009400</td>\n",
              "      <td>1.248939</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.826772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>1.245789</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.859504</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.825397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>1.269688</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.826772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>1.294815</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.865546</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.824000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.009800</td>\n",
              "      <td>1.301012</td>\n",
              "      <td>0.841155</td>\n",
              "      <td>0.865546</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.824000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3120, training_loss=0.17480973173350764, metrics={'train_runtime': 1202.3337, 'train_samples_per_second': 20.71, 'train_steps_per_second': 2.595, 'total_flos': 3275791385702400.0, 'train_loss': 0.17480973173350764, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = deberta_fine_tuned.predict(test_dataset)\n",
        "# Select the class with the highest probability\n",
        "deberta_ft_predictions = np.argmax((eval_results.predictions > 0).astype(int), axis=1)\n",
        "print(\"DeBERTa v3 fine-tuned predictions: \",deberta_ft_predictions)\n",
        "# evaluate predictions on test set\n",
        "print(\"f1-score : \",sklearn.metrics.f1_score(gpt_labels, deberta_ft_predictions,  average='binary'))\n",
        "print(\"precision-score : \",sklearn.metrics.precision_score(gpt_labels, deberta_ft_predictions,  average='binary'))\n",
        "print(\"recall-score : \",sklearn.metrics.recall_score(gpt_labels, deberta_ft_predictions,  average='binary'))\n",
        "print(\"accuracy-score : \",sklearn.metrics.accuracy_score(gpt_labels, deberta_ft_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "QKOyBM-De2Ru",
        "outputId": "38f7db83-ea18-49b4-9d25-b63a41c6e356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa v3 fine-tuned predictions:  [0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1]\n",
            "f1-score :  0.9\n",
            "precision-score :  0.9\n",
            "recall-score :  0.9\n",
            "accuracy-score :  0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Gemma-2B-it model\n",
        "\n",
        "* Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research <br> and technology used to create the Gemini models. They are text-to-text, decoder-only large language models,<br> available in English, with open weights, pre-trained variants, and instruction-tuned variants.\n",
        "\n",
        "* The bitsandbytes library is a lightweight Python wrapper around CUDA custom functions, <br> in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and 8 & 4-bit quantization functions.\n",
        "\n",
        "* \"It\" in this context stands for instruction tuning, which is a version of Gemma that enables the LLM to interact through structured instructions, thereby responding in terms of \"prompts.\"\n",
        "\n",
        "* https://huggingface.co/docs/transformers/model_doc/gemma"
      ],
      "metadata": {
        "id": "KPf4mR03NsJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set the quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "#Load the model and Tokenizer\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
        "                                                           quantization_config=bnb_config, # Configuration for quantization\n",
        "                                                           device_map={\"\":0}) # Map device to use for model (in this case, GPU index 0)\n",
        "#tokenizer2 = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v84DzgPNvlj",
        "outputId": "8b1919fd-5d44-4217-eedf-5988ad230b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
            "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\n",
            "Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at google/gemma-2b-it and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA on Gemma-2B\n",
        "Low-Rank Adaptation (LoRA) is a PEFT method that decomposes a large matrix <br> into two smaller low-rank matrices in the attention layers. <br> This drastically reduces the number of parameters that need to be fine-tuned.\n",
        "\n",
        "sources :\n",
        "1. https://medium.com/the-ai-forum/instruction-fine-tuning-gemma-2b-on-medical-reasoning-and-convert-the-finetuned-model-into-gguf-844191f8d329\n",
        "2. https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/peft_types.py#L68-L73\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k5JahSleAm8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix library dependencies errors\n",
        "# !pip install git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l3dTcz6kHttq",
        "outputId": "856cda14-f612-4cf4-b11a-43c0bd5aad80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c\r\n",
            "  Cloning https://github.com/huggingface/trl.git (to revision 7630f877f91c556d9e5a3baa4b6e2894d90ff84c) to /tmp/pip-req-build-0n1f2pq3\r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-req-build-0n1f2pq3\n",
            "  Running command git rev-parse -q --verify 'sha^7630f877f91c556d9e5a3baa4b6e2894d90ff84c'\n",
            "  Running command git fetch -q https://github.com/huggingface/trl.git 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n",
            "  Running command git checkout -q 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n",
            "  Resolved https://github.com/huggingface/trl.git to commit 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n",
            "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from trl==0.7.12.dev0) (2.2.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from trl==0.7.12.dev0) (4.40.2)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from trl==0.7.12.dev0) (1.26.4)\n",
            "Requirement already satisfied: accelerate in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from trl==0.7.12.dev0) (0.30.1)\n",
            "Requirement already satisfied: datasets in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from trl==0.7.12.dev0) (2.18.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from trl==0.7.12.dev0) (0.8.4)\n",
            "Requirement already satisfied: filelock in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (2.11.3)\n",
            "Requirement already satisfied: fsspec in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl==0.7.12.dev0) (12.4.127)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (4.66.2)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from tyro>=0.5.11->trl==0.7.12.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from tyro>=0.5.11->trl==0.7.12.dev0) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from tyro>=0.5.11->trl==0.7.12.dev0) (1.7.1)\n",
            "Requirement already satisfied: eval-type-backport>=0.1.3 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from tyro>=0.5.11->trl==0.7.12.dev0) (0.2.0)\n",
            "Requirement already satisfied: psutil in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from accelerate->trl==0.7.12.dev0) (5.9.8)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets->trl==0.7.12.dev0) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets->trl==0.7.12.dev0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets->trl==0.7.12.dev0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets->trl==0.7.12.dev0) (2.2.1)\n",
            "Requirement already satisfied: xxhash in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets->trl==0.7.12.dev0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets->trl==0.7.12.dev0) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from datasets->trl==0.7.12.dev0) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (2.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from jinja2->torch>=1.4.0->trl==0.7.12.dev0) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from sympy->torch>=1.4.0->trl==0.7.12.dev0) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/nicola/anaconda3/envs/nlp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.12.dev0) (1.16.0)\n",
            "Building wheels for collected packages: trl\n",
            "  Building wheel for trl (pyproject.toml) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Created wheel for trl: filename=trl-0.7.12.dev0-py3-none-any.whl size=173433 sha256=77a76c51a948f8cba9e85ffd1bc09d612984c491115d93b35fc0fc00ddba01a3\n",
            "  Stored in directory: /home/nicola/.cache/pip/wheels/9a/bc/37/37a37c7224d6e84e02b55023b267d1f0adcedfb6c959693144\n",
            "Successfully built trl\n",
            "Installing collected packages: trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.7.10\n",
            "    Uninstalling trl-0.7.10:\n",
            "      Successfully uninstalled trl-0.7.10\n",
            "Successfully installed trl-0.7.12.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantizing only the linear modules can lead to a significant reduction in model size and computational requirements,<br> which is particularly important for our application where computational resources are limited."
      ],
      "metadata": {
        "id": "xVrrYe-whusK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_linear_names(model):\n",
        "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "  lora_module_names = set()\n",
        "  for name, module in model.named_modules():\n",
        "    if isinstance(module, cls):\n",
        "      names = name.split('.')\n",
        "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "      lora_module_names.remove('lm_head')\n",
        "  return list(lora_module_names)\n",
        "#\n",
        "modules = find_all_linear_names(model)\n",
        "print(modules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJdj_Z_XDECL",
        "outputId": "a24c8877-0853-41a1-969c-d12404ba763a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['q_proj', 'up_proj', 'down_proj', 'gate_proj', 'v_proj', 'k_proj', 'o_proj']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepares a model for optimized fine-tuning using techniques for efficient memory management during training. <br> We use gradient checkpointing to optimize memory usage, configures the model for quantization-aware training (KBIT) <br> using the Lora framework, and applies Parameter-Efficient Fine-Tuning (PEFT) to optimize the model for a sequential classification task.\n",
        "\n",
        "The resulting model is no longer quantized but has been prepared to be **executable** with full precision, while retaining the performance optimizations introduced during KBIT and PEFT."
      ],
      "metadata": {
        "id": "Cd3plpTD3ZRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable gradient checkpointing for memory optimization during training\n",
        "model.gradient_checkpointing_enable()\n",
        "# Prepare the model for kbit training, which involves preparing adapters for quantization-aware training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(model)\n",
        "\n",
        "# Define LoraConfig for quantization-aware training\n",
        "lora_config = LoraConfig(\n",
        "    r=64, # Lora attention dimension (the “rank”).\n",
        "    lora_alpha=32, # The alpha parameter for Lora scaling.\n",
        "    target_modules=modules, # List of linear modules in the model\n",
        "    lora_dropout=0.05, # The dropout probability for Lora layers.\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\" # The task type is sequence classification\n",
        ")\n",
        "\n",
        "# Get the model ready for fine-tuning\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKPbBvnlArww",
        "outputId": "969a6ca9-7f50-4ced-81b2-81f8a1428357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GemmaForSequenceClassification(\n",
            "  (model): GemmaModel(\n",
            "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x GemmaDecoderLayer(\n",
            "        (self_attn): GemmaSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): GemmaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): GemmaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): GemmaRMSNorm()\n",
            "        (post_attention_layernorm): GemmaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): GemmaRMSNorm()\n",
            "  )\n",
            "  (score): Linear(in_features=2048, out_features=2, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.add_adapter(peft_config = lora_config, adapter_name=\"adapter_1\")\n",
        "# # Print the keys of the model to verify the adapter is added\n",
        "# print(model.state_dict().keys())"
      ],
      "metadata": {
        "id": "0_hjvUr8NelN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The number of parameters\n",
        "\n",
        "* To be able to fine-tuning Gemma-2b on our dataset without special type of GPU,<br>  we must select a subset of the total parameters (over 2 bilion).\n",
        "<br> <br>\n",
        "* In the Percentage field, we display the number of parameters <br>that we will consider relative to the total parameters of Gemma 2b."
      ],
      "metadata": {
        "id": "Mioa4CYZeS-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nR4GuWLDgfA",
        "outputId": "2a8bbbb7-f4b1-4429-a268-16b5470f9e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable: 78450688 | total: 2584627200 | Percentage: 3.0353%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redo Tokenizzation\n",
        "In this section, we will re-run tokenization on the dataset <br>changing the type of tokenizer, namely GemmaTokenizerFast\n"
      ],
      "metadata": {
        "id": "QbFYsWs0EllN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling parallelism to avoid deadlocks\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "gemma_tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
        "#gemma_tokenized_datasets = dataset.map(tokenize_function, batched=True) # map applies the tokenize_function to all the rows of the dataset.\n",
        "gemma_tokenized_datasets = dataset.map(lambda x: tokenize_function(x, gemma_tokenizer), batched=True)\n",
        "print(gemma_tokenized_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJAHFkl8DpAp",
        "outputId": "e2e4a2da-0946-4993-af00-0128db6b28b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████| 277/277 [00:00<00:00, 5408.73 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 2490\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 277\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text1', 'text2', 'label', 'idx', 'label_text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Info about the Tokenizer\n",
        "\n",
        "Gemma uses a fast tokenizer based on Byte-Pair-Encoding (BPE).<br>\n",
        "It is designed to be efficient and scalable, allowing us to tokenize large amounts of text quickly and easily."
      ],
      "metadata": {
        "id": "ED1NdvQgBjUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(gemma_tokenizer).partition('\\n')[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXxIwps5E0gP",
        "outputId": "6a827c2f-8991-41c7-9d44-73cfb73b2874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"GemmaTokenizerFast(name_or_path='google/gemma-2b-it', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\", '\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma-2b-it before fine-tuning\n",
        "\n",
        "At this stage, we will use the newly loaded Gemma-2b-it model to perform inference on the test set.<br> We will then compare its predictions with those of ChatGPT."
      ],
      "metadata": {
        "id": "PrAIgv-gpRcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create test set with gemma tokenized dataset\n",
        "test_dataset1 = get_test_data(gemma_tokenized_datasets)"
      ],
      "metadata": {
        "id": "zSgt4W4F9tJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_base_predictions = []\n",
        "\n",
        "# set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# generate predictions on test set\n",
        "# disable gradient calculation to save memory and improve performance during inference\n",
        "with torch.no_grad():\n",
        "  # convert the example's input_ids and attention_mask into torch tensors\n",
        "    for example in test_dataset1:\n",
        "        input_ids = torch.tensor([example['input_ids']])#.to(device)\n",
        "        attention_mask = torch.tensor([example['attention_mask']])#.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        # extract the logits from the model's outputs\n",
        "        logits = outputs.logits\n",
        "        # determine the index of the class with the highest logit value\n",
        "        predictions = torch.argmax(logits, dim=-1).cpu().item()\n",
        "        gemma_base_predictions.append(predictions)\n",
        "\n",
        "print(\"Chat-gpt predictions: \",gpt_labels)\n",
        "print(\"Gemma 2b-it predictions: \",gemma_base_predictions)\n",
        "print(\"f1-score : \",sklearn.metrics.f1_score(gpt_labels, gemma_base_predictions,  average='binary'))\n",
        "print(\"precision-score : \",sklearn.metrics.precision_score(gpt_labels, gemma_base_predictions,  average='binary'))\n",
        "print(\"recall-score : \",sklearn.metrics.recall_score(gpt_labels, gemma_base_predictions,  average='binary'))\n",
        "print(\"accuracy-score : \",sklearn.metrics.accuracy_score(gpt_labels, gemma_base_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oit3dC7hpa1c",
        "outputId": "cecdc1fd-5e58-4a3b-a6dc-cef7ff062f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat-gpt predictions:  [0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "Gemma 2b-it predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "f1-score :  0.6666666666666666\n",
            "precision-score :  0.5\n",
            "recall-score :  1.0\n",
            "accuracy-score :  0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing Hyperparameters with Optuna: From Experiments to Fine-Tuning\n",
        "\n",
        "In this section, we redefine the hyperparameters to conduct a new study with Optuna, <br> very similar to the one previously conducted, <br> but reducing the number of epochs due to the long fine-tuning times of Gemma,<br>\n",
        "and decreasing the number of eval_steps to maintain a consistent history log despite the fewer epochs.\n"
      ],
      "metadata": {
        "id": "6KDGimSKtbn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective2(trial):\n",
        "  # Define hyperparameters to search over\n",
        "  hparams = optuna_hp_space(trial)\n",
        "\n",
        "  # Set up training arguments with current hyperparameters\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=f\"./Gemma2b_rte_steps_50_batch_8/trial_{trial.number}\",\n",
        "      evaluation_strategy=\"steps\", #The evaluation strategy to adopt during training, \"epoch\" or \"steps\": Evaluation is done (and logged) every eval_steps or epoch\n",
        "      eval_steps= 50,# eval_steps is a parameter that specifies how often the model should evaluate its performance on the validation dataset during training, measured in terms of training steps\n",
        "      per_device_train_batch_size =  8, # Define the batch size for training on each device (e.g., GPU).\n",
        "      #gradient_accumulation_steps = 2, # Allows the gradients to be accumulated over multiple batches before updating the model's weights. Therefore, logging, evaluation, save will be conducted every gradient_accumulation_steps * xxx_step training examples.\n",
        "      learning_rate = hparams[\"learning_rate\"],\n",
        "      weight_decay = hparams[\"weight_decay\"],\n",
        "      num_train_epochs = 3, # num_train_epochs is a parameter that specifies the number of epochs (complete passes through the entire training dataset) to train the model.\n",
        "      logging_steps = 50,\n",
        "      logging_dir = f\"./Gemma2b_rte_steps_50_batch_8/trial_{trial.number}\",\n",
        "      overwrite_output_dir = True,\n",
        "      disable_tqdm = False,  # Enable tqdm progress bars\n",
        "  )\n",
        "  # add callbacks to stop training if the maximization of the objective function gives bad result after \"patience\" epochs\n",
        "  # callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "  # Initialize the Trainer\n",
        "  trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=gemma_tokenized_datasets[\"train\"],\n",
        "    eval_dataset=gemma_tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "  # Fine-tune the model with Cross-Entropy loss\n",
        "  trainer.train()\n",
        "\n",
        "  # Evaluate the model and return the validation metric\n",
        "  eval_results = trainer.evaluate()\n",
        "\n",
        "  return eval_results[\"eval_accuracy\"]"
      ],
      "metadata": {
        "id": "GLnJ_4YzAytw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results of Gemma Model Experiments\n",
        "\n",
        "Both experiments show a similar trend, with the training loss decreasing fairly rapidly <br> while the validation loss follows an irregular pattern. <br> In this scenario, we consider the experiment that provides better accuracy on the validation set."
      ],
      "metadata": {
        "id": "f3mg6XioOnfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform hyperparameter search with Optuna\n",
        "start_time = time.time()\n",
        "study2 = optuna.create_study(study_name='hyper-parameter-search', direction=\"maximize\", sampler=TPESampler(seed=42))\n",
        "study2.optimize(objective2, n_trials = 2)\n",
        "end_time = time.time()\n",
        "study_time = end_time - start_time\n",
        "\n",
        "# Gives the best validation accuracy value\n",
        "print(\"study best value is : \\n\")\n",
        "print(study2.best_value)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Gives the best hyperparameter values to get the best validation accuracy value\n",
        "print(\"study best parameters are : \\n\")\n",
        "print(study2.best_params)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Return info about best Trial such as start and end datetime, hyperparameters\n",
        "print(\"study best trial is : \\n\")\n",
        "print(study2.best_trial)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# study time\n",
        "print(\"time to complete study : \", study_time)\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IVH-rC6qBEF4",
        "outputId": "033e90b1-d369-4f00-adba-cc9d64c8b9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-26 18:18:14,758] A new study created in memory with name: hyper-parameter-search\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 1:22:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.810800</td>\n",
              "      <td>0.760166</td>\n",
              "      <td>0.559567</td>\n",
              "      <td>0.521951</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.636905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.671200</td>\n",
              "      <td>0.652767</td>\n",
              "      <td>0.617329</td>\n",
              "      <td>0.650602</td>\n",
              "      <td>0.412214</td>\n",
              "      <td>0.504673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.613000</td>\n",
              "      <td>0.600949</td>\n",
              "      <td>0.700361</td>\n",
              "      <td>0.653846</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.710801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.528100</td>\n",
              "      <td>0.504388</td>\n",
              "      <td>0.761733</td>\n",
              "      <td>0.798165</td>\n",
              "      <td>0.664122</td>\n",
              "      <td>0.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.474500</td>\n",
              "      <td>0.506444</td>\n",
              "      <td>0.772563</td>\n",
              "      <td>0.761538</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.758621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.569800</td>\n",
              "      <td>0.488113</td>\n",
              "      <td>0.754513</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.755725</td>\n",
              "      <td>0.744361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.385000</td>\n",
              "      <td>0.487380</td>\n",
              "      <td>0.776173</td>\n",
              "      <td>0.737931</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.775362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.324700</td>\n",
              "      <td>0.534746</td>\n",
              "      <td>0.776173</td>\n",
              "      <td>0.748201</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.770370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.360800</td>\n",
              "      <td>0.583356</td>\n",
              "      <td>0.801444</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.885496</td>\n",
              "      <td>0.808362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.375400</td>\n",
              "      <td>0.547514</td>\n",
              "      <td>0.790614</td>\n",
              "      <td>0.755245</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.788321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.332100</td>\n",
              "      <td>0.536731</td>\n",
              "      <td>0.783394</td>\n",
              "      <td>0.744828</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.782609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.278700</td>\n",
              "      <td>0.533001</td>\n",
              "      <td>0.779783</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.782918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.188700</td>\n",
              "      <td>0.674367</td>\n",
              "      <td>0.794224</td>\n",
              "      <td>0.753425</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.794224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.794896</td>\n",
              "      <td>0.790614</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.778626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.826289</td>\n",
              "      <td>0.790614</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.778626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.170900</td>\n",
              "      <td>0.847140</td>\n",
              "      <td>0.787004</td>\n",
              "      <td>0.760870</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.780669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.159400</td>\n",
              "      <td>0.864441</td>\n",
              "      <td>0.790614</td>\n",
              "      <td>0.774436</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.780303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.295100</td>\n",
              "      <td>0.837022</td>\n",
              "      <td>0.797834</td>\n",
              "      <td>0.769784</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.792593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:48]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-26 19:41:29,180] Trial 0 finished with value: 0.7978339350180506 and parameters: {'learning_rate': 1.5355286838886852e-05, 'weight_decay': 0.0007969454818643932}. Best is trial 0 with value: 0.7978339350180506.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 1:22:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.286600</td>\n",
              "      <td>0.772443</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.783217</td>\n",
              "      <td>0.854962</td>\n",
              "      <td>0.817518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.268300</td>\n",
              "      <td>1.271841</td>\n",
              "      <td>0.783394</td>\n",
              "      <td>0.726115</td>\n",
              "      <td>0.870229</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.365800</td>\n",
              "      <td>0.784362</td>\n",
              "      <td>0.787004</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.893130</td>\n",
              "      <td>0.798635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.231000</td>\n",
              "      <td>1.011187</td>\n",
              "      <td>0.783394</td>\n",
              "      <td>0.741497</td>\n",
              "      <td>0.832061</td>\n",
              "      <td>0.784173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.832194</td>\n",
              "      <td>0.797834</td>\n",
              "      <td>0.773723</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.791045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.409800</td>\n",
              "      <td>0.872578</td>\n",
              "      <td>0.787004</td>\n",
              "      <td>0.746575</td>\n",
              "      <td>0.832061</td>\n",
              "      <td>0.787004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.151700</td>\n",
              "      <td>1.122513</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.821138</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.795276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>1.011149</td>\n",
              "      <td>0.815884</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.804598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.039200</td>\n",
              "      <td>1.135136</td>\n",
              "      <td>0.830325</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.870229</td>\n",
              "      <td>0.829091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.127900</td>\n",
              "      <td>1.128250</td>\n",
              "      <td>0.826715</td>\n",
              "      <td>0.802920</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.820896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.076200</td>\n",
              "      <td>1.028023</td>\n",
              "      <td>0.837545</td>\n",
              "      <td>0.798611</td>\n",
              "      <td>0.877863</td>\n",
              "      <td>0.836364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.045400</td>\n",
              "      <td>1.235022</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.758170</td>\n",
              "      <td>0.885496</td>\n",
              "      <td>0.816901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.054800</td>\n",
              "      <td>1.260729</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.847328</td>\n",
              "      <td>0.807273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.256578</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.812030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.294144</td>\n",
              "      <td>0.819495</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.812030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>1.310055</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.780142</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.808824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.318770</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.810606</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.813688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>1.304072</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.788321</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.805970</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:48]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-26 21:04:49,321] Trial 1 finished with value: 0.8122743682310469 and parameters: {'learning_rate': 4.4803926826840614e-05, 'weight_decay': 0.00015751320499779721}. Best is trial 1 with value: 0.8122743682310469.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study best value is : \n",
            "\n",
            "0.8122743682310469\n",
            "\n",
            "\n",
            "\n",
            "study best parameters are : \n",
            "\n",
            "{'learning_rate': 4.4803926826840614e-05, 'weight_decay': 0.00015751320499779721}\n",
            "\n",
            "\n",
            "\n",
            "study best trial is : \n",
            "\n",
            "FrozenTrial(number=1, state=TrialState.COMPLETE, values=[0.8122743682310469], datetime_start=datetime.datetime(2024, 6, 26, 19, 41, 29, 180920), datetime_complete=datetime.datetime(2024, 6, 26, 21, 4, 49, 321514), params={'learning_rate': 4.4803926826840614e-05, 'weight_decay': 0.00015751320499779721}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.0001, log=True, low=5e-06, step=None), 'weight_decay': FloatDistribution(high=0.001, log=True, low=1e-05, step=None)}, trial_id=1, value=None)\n",
            "\n",
            "\n",
            "\n",
            "time to complete study :  9994.563913822174\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma-2b-it after fine-tuning\n",
        "\n",
        "Now we will proceed with fine-tuning, which means we will take the Gemma-2b-it model <br> and refine it by training it on our labeled SetFit/rte dataset so that it becomes more adept at performing the RTE task. <br> Then we will compare its predictions with those of ChatGPT."
      ],
      "metadata": {
        "id": "tCE1KIQ_z5OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best parameters found\n",
        "study2_params = {'learning_rate': 4.4803926826840614e-05, 'weight_decay': 0.00015751320499779721}\n",
        "# Set up training arguments with current hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    evaluation_strategy=\"steps\", #The evaluation strategy to adopt during training, \"epoch\" or \"steps\": Evaluation is done (and logged) every eval_steps or epoch\n",
        "    output_dir=f\"./Gemma2b_rte_steps_50_batch_8/best_param/\",\n",
        "    eval_steps= 50,# eval_steps is a parameter that specifies how often the model should evaluate its performance on the validation dataset during training, measured in terms of training steps\n",
        "    per_device_train_batch_size =  8, # Define the batch size for training on each device (e.g., GPU).\n",
        "    learning_rate =  study2_params['learning_rate'],\n",
        "    weight_decay =  study2_params['weight_decay'],\n",
        "    num_train_epochs =  3, # num_train_epochs is a parameter that specifies the number of epochs (complete passes through the entire training dataset) to train the model.\n",
        "    logging_steps = 50,\n",
        "    overwrite_output_dir = True,\n",
        "    disable_tqdm = False,  # Enable tqdm progress bars\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "gemma2b_fine_tuned = Trainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=gemma_tokenized_datasets[\"train\"],\n",
        "  eval_dataset=gemma_tokenized_datasets[\"validation\"],\n",
        "  compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Fine-tune the model with Cross-Entropy loss\n",
        "gemma2b_fine_tuned.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "Yt9gLcSEz9oi",
        "outputId": "b334918b-1716-417d-c22b-c5b496f3cdfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 1:22:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.779400</td>\n",
              "      <td>0.651700</td>\n",
              "      <td>0.646209</td>\n",
              "      <td>0.681319</td>\n",
              "      <td>0.473282</td>\n",
              "      <td>0.558559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.630900</td>\n",
              "      <td>0.697764</td>\n",
              "      <td>0.693141</td>\n",
              "      <td>0.626374</td>\n",
              "      <td>0.870229</td>\n",
              "      <td>0.728435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.647900</td>\n",
              "      <td>0.593402</td>\n",
              "      <td>0.718412</td>\n",
              "      <td>0.644809</td>\n",
              "      <td>0.900763</td>\n",
              "      <td>0.751592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.505500</td>\n",
              "      <td>0.524675</td>\n",
              "      <td>0.765343</td>\n",
              "      <td>0.706250</td>\n",
              "      <td>0.862595</td>\n",
              "      <td>0.776632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.442400</td>\n",
              "      <td>0.454734</td>\n",
              "      <td>0.794224</td>\n",
              "      <td>0.772059</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.786517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.524100</td>\n",
              "      <td>0.468222</td>\n",
              "      <td>0.797834</td>\n",
              "      <td>0.781955</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.787879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.281900</td>\n",
              "      <td>0.611243</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.743750</td>\n",
              "      <td>0.908397</td>\n",
              "      <td>0.817869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.231000</td>\n",
              "      <td>0.780572</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>0.801527</td>\n",
              "      <td>0.795455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.288700</td>\n",
              "      <td>0.762418</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.734940</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.821549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.263000</td>\n",
              "      <td>0.651761</td>\n",
              "      <td>0.805054</td>\n",
              "      <td>0.758389</td>\n",
              "      <td>0.862595</td>\n",
              "      <td>0.807143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.224500</td>\n",
              "      <td>0.667532</td>\n",
              "      <td>0.801444</td>\n",
              "      <td>0.763889</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.234000</td>\n",
              "      <td>0.714172</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.759494</td>\n",
              "      <td>0.916031</td>\n",
              "      <td>0.830450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.075100</td>\n",
              "      <td>0.792805</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.780142</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.808824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.023100</td>\n",
              "      <td>0.907276</td>\n",
              "      <td>0.808664</td>\n",
              "      <td>0.791045</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.062400</td>\n",
              "      <td>0.938136</td>\n",
              "      <td>0.823105</td>\n",
              "      <td>0.797101</td>\n",
              "      <td>0.839695</td>\n",
              "      <td>0.817844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.071400</td>\n",
              "      <td>0.950639</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.796992</td>\n",
              "      <td>0.809160</td>\n",
              "      <td>0.803030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.039900</td>\n",
              "      <td>0.941544</td>\n",
              "      <td>0.812274</td>\n",
              "      <td>0.792593</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.804511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.046100</td>\n",
              "      <td>0.958937</td>\n",
              "      <td>0.801444</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.785992</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=936, training_loss=0.28786015663391507, metrics={'train_runtime': 4939.1751, 'train_samples_per_second': 1.512, 'train_steps_per_second': 0.19, 'total_flos': 2.3640167153664e+16, 'train_loss': 0.28786015663391507, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_eval_results = gemma2b_fine_tuned.predict(test_dataset1)\n",
        "#print(gemma_eval_results)\n",
        "# Select the class with the highest probability\n",
        "gemma_fn_preds = np.argmax((gemma_eval_results.predictions > 0).astype(int), axis=1)\n",
        "print(\"Chat-gpt predictions: \",gpt_labels)\n",
        "print(\"Gemma 2b-it fine-tuned predictions: \",gemma_fn_preds)\n",
        "print(\"f1-score : \",sklearn.metrics.f1_score(gpt_labels, gemma_fn_preds,  average='binary'))\n",
        "print(\"precision-score : \",sklearn.metrics.precision_score(gpt_labels, gemma_fn_preds,  average='binary'))\n",
        "print(\"recall-score : \",sklearn.metrics.recall_score(gpt_labels, gemma_fn_preds,  average='binary'))\n",
        "print(\"accuracy-score : \",sklearn.metrics.accuracy_score(gpt_labels, gemma_fn_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Ww2yRzu30bGY",
        "outputId": "181c6f03-572c-4681-afac-d7f07ab4371f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat-gpt predictions:  [0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "Gemma 2b-it fine-tuned predictions:  [0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1]\n",
            "f1-score :  0.9\n",
            "precision-score :  0.9\n",
            "recall-score :  0.9\n",
            "accuracy-score :  0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "Both models perform well in solving the following natural language processing task, RTE. <br> It should be noted that, in terms of training times, DeBERTa is much faster and less memory-intensive compared to Gemma. <br> However, we did not fully leverage Gemma's potential as we only selected its linear modules and not the entire model with 2 billion hyperparameters.<br> Finally, in both cases, we can observe the significant difference in performance before and after fine-tuning, highlighting the importance of applying this technique to enhance the model's understanding of a specific task."
      ],
      "metadata": {
        "id": "5yWdab-xLn8N"
      }
    }
  ]
}